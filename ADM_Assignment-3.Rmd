---
title: "ADM Assignment-3"
author: "Spandana Sodadasi"
date: "2024-05-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL)
knitr::opts_chunk$set(warning = FALSE)
```

### Summary:-

### PART-A

1. What is the difference between SVM with hard margin and soft margin?              

Answer: Support Vector Machine (SVM) is a powerful machine learning algorithm designed to find the decision boundary with the widest margin, making it robust to overfitting and noise. It works by identifying the optimal hyperplane that separates different classes in the input space. SVM provides two margin configurations: hard margin and soft margin, each managing misclassifications and error tolerance in distinct ways.

Hard Margin SVM: In hard margin SVM, the decision boundary strictly separates all data points without any classification errors, aiming for a perfect fit. However, in real world scenarios, data is often not linearly separable, making hard margins impractical. They may overfit when dealing with noisy or non-linearly separable data, as it strives for a perfect fit that will fit best on training data and might not generalize well to new, unseen data. Soft margin SVM addresses the limitations of hard margins by introducing flexibility in the decision boundary.

Soft Margin SVM: Soft margin SVM strikes a balance between margin width and classification errors, controlled by the regularization parameter \(C\). It allows a certain degree of misclassification, promoting better generalization. By introducing slack variables, it permits some data points to fall within the margin or on the wrong side of the hyperplane, providing flexibility in handling noisy data. The pivotal role of the regularization parameter \(C\), cannot be overstated: a small \(C\) value results in a wider margin with greater tolerance for misclassifications, while a large \(C\)  value yields a narrower margin with less tolerance for errors.

$$Min_{w,ξ} = ||W||^2 + C ∑_i^Nξ_i$$

Where,

\(W\) = weights

\(C\) =  regularization parameter

\(ξ\) = Slack variable

Conclusion: While hard margin SVM strives for perfect classification, soft margin SVM offers a more flexible and adaptive solution by allowing for some degree of misclassification and adjusting the margin width based on the regularization parameter. Soft margin SVM better accommodates the complexities of real-world data, making it a preferred choice for practical machine learning applications.


2. What is the role of the cost parameter, \(C\), in SVM (with soft margin) classifiers?

Answer: In soft margin SVM classification, the parameter '\(C\)' plays a crucial role in balancing two key objectives which is maximizing the margin between classes and minimizing misclassification errors. It serves as the determining factor for finding the right equilibrium between widening the margin and allowing a certain level of misclassification to improve generalization.

Large '\(C\)' Value: A higher '\(C\)' value imposes a significant penalty on misclassification errors, resulting in a narrower margin. While this may closely fit the training data, there's a risk of overfitting, limiting the model's ability to generalize to new data. It essentially represents a stricter classifier, similar to hard margin SVMs. If \(C\) = ∞  then it is called as a hard margin.

Small '\(C\)' Value: Conversely, a lower '\(C\)' value assigns less importance to misclassification errors, allowing for a wider margin between classes. This flexibility can enhance generalization, but excessively small '\(C\)' values may lead to underfitting and sub-optimal performance.

Support Vector Machines face the challenge of balancing margin maximization and error minimization, defining the essence of the '\(C\)' parameter in achieving optimal performance while avoiding overfitting and underfitting.

3. Will the following perceptron be activated (2.8 is the activation threshold)? When the provided inputs and their respective mappings are outlined as 0.1 is mapped to 0.8, The threshold is set at 2.8, 11.1 is mapped to -0.2.
```{r}
Net = (0.1)*(0.8) + 11.1*(-0.2)
Net
```

Answer: Therefore, as the weighted sum of inputs is -2.14 which is lower than the activation threshold 2.8, the perceptron will remain inactive.

4. What is the role of alpha, the learning rate in the delta rule?

Answer: The delta rule is a fundamental algorithm in machine learning that is essential for refining the accuracy of neural network models through iterative weight adjustments. It operates by continuously updating weights according to the difference between predicted and actual outputs, progressively minimizing errors. Central to the delta rule is the learning rate parameter, denoted as alpha (α), which controls the magnitude of weight adjustments during training. Alpha plays a crucial role in determining how quickly the neural network converges towards the optimal solution.

Higher Alpha (Large α) Value: When alpha is set high, weights change rapidly. This accelerates the learning process, allowing the model to converge to a solution quickly. However, the fast pace of weight updates increases the risk of instability and uncertain behavior in the model.

Lower Alpha (Small α) Value: With a lower alpha, weight changes occur more gradually. This smoother adjustment fosters stability in the learning process, reducing the risk of uncertainty. However, the slower pace of weight updates may lead to longer convergence times and slower training.

Therefore, selecting an appropriate alpha is pivotal for effective learning. Initiating with a higher learning rate allows for swift adaptation, while gradually reducing it ensures careful refinement and optimal performance. Striking this balance make sure that the model learns efficiently without rushing or getting stuck.


### PART-B

1. Build a linear SVM regression model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to “svmLinear”. What is the R-squared of the model?

Answer: The R squared value of the model is 0.3344651.

2. Customize the search grid by checking the model’s performance for C parameter of 0.1, .5, 1 and 10 using 2 repeats of 5-fold cross validation.

Answer: We can observe that the final value selected for the model was \(C\)=1 which is resulting in the smallest RMSE value of 2.318770 and the highest \(R^2\) value of 0.3186781 when compared to all other parameters. Therefore, in this scenario the optimal choice is \(C\)=1.

3. Train a neural network model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to “nnet”. What is the R-square of the model with the best hyper parameters (using default caret search grid) – hint: don’t forget to scale the data.

Answer: The selected model has a size parameter set to 1 and a decay parameter of 1e-04, identified as the most optimal choice according to RMSE. However, the \(R^2\) value for this model is marked as "NAN" (Not a Number). The closest \(R^2\) value in this case is 0.3316546, which corresponds to a model with a decay parameter of 1e-01 and an RMSE of 7.061286.

4. Consider the following input:

• Sales=9, 

• Price=6.54

• Population=124

• Advertising=0

• Age=76

• Income= 110

• Education=10

What will be the estimated Sales for this record using the above neuralnet model?

Answer: The estimated sales value for this record using the above neural network model is '1'. 

***

### Problem Statement:-

### Part A

QA1. What is the difference between SVM with hard margin and soft margin? 

QA2. What is the role of the cost parameter, C, in SVM (with soft margin) classifiers? 

QA3. Will the following perceptron be activated (2.8 is the activation threshold) 

QA4. What is the role of alpha, the learning rate in the delta rule?

### Part B

QB1. Build a linear SVM regression model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to “svmLinear”. What is the R-squared of the model? 

QB2. Customize the search grid by checking the model’s performance for C parameter of 0.1, .5, 1 and 10 using 2 repeats of 5-fold cross validation. 

QB3. Train a neural network model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to “nnet”. What is the R-square of the model with the best hyper parameters (using default caret search grid) – hint: don’t forget to scale the data. 

QB4. Consider the following input:

• Sales=9

• Price=6.54

• Population=124

• Advertising=0

• Age=76

• Income= 110

• Education=10

What will be the estimated Sales for this record using the above neural net model? 

***

### PART-A

1. What is the difference between SVM with hard margin and soft margin?              

Support Vector Machine (SVM) is a powerful machine learning algorithm designed to find the decision boundary with the widest margin, making it robust to overfitting and noise. It works by identifying the optimal hyperplane that separates different classes in the input space. SVM provides two margin configurations: hard margin and soft margin, each managing misclassifications and error tolerance in distinct ways.

Hard Margin SVM: In hard margin SVM, the decision boundary strictly separates all data points without any classification errors, aiming for a perfect fit. However, in real world scenarios, data is often not linearly separable, making hard margins impractical. They may overfit when dealing with noisy or non-linearly separable data, as it strives for a perfect fit that will fit best on training data and might not generalize well to new, unseen data. Soft margin SVM addresses the limitations of hard margins by introducing flexibility in the decision boundary.

Soft Margin SVM: Soft margin SVM strikes a balance between margin width and classification errors, controlled by the regularization parameter \(C\). It allows a certain degree of misclassification, promoting better generalization. By introducing slack variables, it permits some data points to fall within the margin or on the wrong side of the hyperplane, providing flexibility in handling noisy data. The pivotal role of the regularization parameter \(C\), cannot be overstated: a small \(C\) value results in a wider margin with greater tolerance for misclassifications, while a large \(C\)  value yields a narrower margin with less tolerance for errors.

$$Min_{w,ξ} = ||W||^2 + C ∑_i^Nξ_i$$

Where,

\(W\) = weights

\(C\) =  regularization parameter

\(ξ\) = Slack variable

Conclusion: While hard margin SVM strives for perfect classification, soft margin SVM offers a more flexible and adaptive solution by allowing for some degree of misclassification and adjusting the margin width based on the regularization parameter. Soft margin SVM better accommodates the complexities of real-world data, making it a preferred choice for practical machine learning applications.

2. What is the role of the cost parameter, \(C\), in SVM (with soft margin) classifiers?

In soft margin SVM classification, the parameter '\(C\)' plays a crucial role in balancing two key objectives which is maximizing the margin between classes and minimizing misclassification errors. It serves as the determining factor for finding the right equilibrium between widening the margin and allowing a certain level of misclassification to improve generalization.

Large '\(C\)' Value: A higher '\(C\)' value imposes a significant penalty on misclassification errors, resulting in a narrower margin. While this may closely fit the training data, there's a risk of overfitting, limiting the model's ability to generalize to new data. It essentially represents a stricter classifier, similar to hard margin SVMs. If \(C\) = ∞  then it is called as a hard margin.

Small '\(C\)' Value: Conversely, a lower '\(C\)' value assigns less importance to misclassification errors, allowing for a wider margin between classes. This flexibility can enhance generalization, but excessively small '\(C\)' values may lead to underfitting and suboptimal performance.

SVMs face the challenge of balancing margin maximization and error minimization, defining the essence of the '\(C\)' parameter in achieving optimal performance while avoiding overfitting and underfitting.


3. Will the following perceptron be activated (2.8 is the activation threshold)? When the provided inputs and their respective mappings are outlined as: 0.1 is mapped to 0.8, The threshold is set at 2.8, 11.1 is mapped to -0.2.

```{r}
Net = (0.1)*(0.8) + 11.1*(-0.2)
Net
```

Therefore, as the weighted sum of inputs is -2.14 which is lower than the activation threshold 2.8, the perceptron will remain inactive.

4. What is the role of alpha, the learning rate in the delta rule?

The delta rule is a fundamental algorithm in machine learning that is essential for refining the accuracy of neural network models through iterative weight adjustments. It operates by continuously updating weights according to the difference between predicted and actual outputs, progressively minimizing errors. Central to the delta rule is the learning rate parameter, denoted as alpha (α), which controls the magnitude of weight adjustments during training. Alpha plays a crucial role in determining how quickly the neural network converges towards the optimal solution.

Higher Alpha (Large α) Value: When alpha is set high, weights change rapidly. This accelerates the learning process, allowing the model to converge to a solution quickly. However, the fast pace of weight updates increases the risk of instability and uncertain behavior in the model.

Lower Alpha (Small α) Value: With a lower alpha, weight changes occur more gradually. This smoother adjustment fosters stability in the learning process, reducing the risk of uncertainty. However, the slower pace of weight updates may lead to longer convergence times and slower training.

Therefore, selecting an appropriate alpha is pivotal for effective learning. Initiating with a higher learning rate allows for swift adaptation, while gradually reducing it ensures careful refinement and optimal performance. Striking this balance make sure that the model learns efficiently without rushing or getting stuck.

### PART-B

1.Loading the required libraries.
```{r}
library(dplyr, warn.conflicts = FALSE)
library(ISLR, warn.conflicts = FALSE)
library(caret, warn.conflicts = FALSE)
library(neuralnet, warn.conflicts = FALSE)
```

2.Loading and reading the dataset.
```{r}
Dataset <- Carseats
dim(Dataset)
```

3.Selecting a subset of the data.
```{r}
Carseats_Filtered <- Dataset %>% select("Sales", "Price", "Advertising", "Population", 
                                        "Age", "Income", "Education")
head(Carseats_Filtered)
```

4.Checking if there are any missing values.
```{r}
Missing_Values <- colSums(is.na(Carseats_Filtered))
Missing_Values
```

There are no missing values in any of the variables.

### Questions:-

1. Build a linear SVM regression model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to “svmLinear”. What is the R-squared of the model?
```{r}
set.seed(1500)

data_split <- createDataPartition(y = Carseats_Filtered$Sales, p= 0.7, list = FALSE)
train_set <- Carseats_Filtered[data_split,]
test_set <- Carseats_Filtered[-data_split,]

trctrl_1 <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

set.seed(1500)

svm_Linear <- train(Sales~., data = train_set, method = 'svmLinear',
                    trControl=trctrl_1, preProcess = c("center", "scale"), 
                    tuneLength = 10)

svm_Linear
```

The R squared value of the model is 0.3344651.

2. Customize the search grid by checking the model’s performance for C parameter of 0.1, .5, 1 and 10 using 2 repeats of 5-fold cross validation.
```{r}
set.seed(1500)

grid <- expand.grid(C = c(0.1,0.5,1,10))

trctrl_2 <- trainControl(method = "repeatedcv", number = 5, repeats = 2)

svm_Linear_Grid <- train(Sales~., data = train_set, method = 'svmLinear', 
                         trControl=trctrl_2, preProcess = c("center", "scale"),
                         tuneGrid = grid, tuneLength = 10)

svm_Linear_Grid
```

We can observe that the final value selected for the model was \(C\)=1 which is resulting in the smallest RMSE value of 2.318770 and the highest \(R^2\) value of 0.3186781 when compared to all other parameters. Therefore, in this scenario the optimal choice is \(C\)=1.

3. Train a neural network model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to “nnet”. What is the R-square of the model with the best hyper parameters (using default caret search grid) – hint: don’t forget to scale the data.
```{r}
set.seed(1500)

crossfolds <- trainControl(method = 'cv', number = 10, verboseIter = FALSE)

NN_Model <- train(Sales~., data = train_set, method = 'nnet', 
                  trControl= crossfolds, preProcess = c("center", "scale"), 
                  trace = FALSE)

NN_Model
```

The selected model has a size parameter set to 1 and a decay parameter of 1e-04, identified as the most optimal choice according to RMSE. However, the \(R^2\) value for this model is marked as "NAN" (Not a Number). The closest \(R^2\) value in this case is 0.3316546, which corresponds to a model with a decay parameter of 1e-01 and an RMSE of 7.061286.

4. Consider the following input: Sales=9, Price=6.54, Population=124, Advertising=0, Age=76, Income= 110, Education=10
What will be the estimated Sales for this record using the above neural net model?
```{r}
set.seed(1500)

Data = data.frame(Sales=9, Price = 6.54, Population = 124, Advertising = 0, Age = 76,
                  Income = 110, Education = 10)

Sales_Value = predict(NN_Model, Data)
cat(Sales_Value)
```
The estimated sales value for this record using the above neural network model is '1'.


